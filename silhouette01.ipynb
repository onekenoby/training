{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the video\n",
    "video_path = \"myVideo.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video loaded successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Define the video writer to save the output video with silhouettes\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter(\n",
    "    'output_with_silhouette.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "# Load a pre-trained face classifier (using Haar cascade)\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to grayscale (needed for face detection)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # Draw the silhouette lines for each detected face\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Calculate key points for silhouette lines\n",
    "        center_x = x + w // 2\n",
    "        neck_y = y + h  # Approximate neck position below the face\n",
    "        # Approximate position of eyes (1/3 from top of face)\n",
    "        eyes_y = y + h // 3\n",
    "\n",
    "        # Draw vertical line for neck and nose\n",
    "        cv2.line(frame, (center_x, y), (center_x, neck_y),\n",
    "                 (0, 0, 0), thickness=2)\n",
    "\n",
    "        # Draw horizontal line for eyes\n",
    "        cv2.line(frame, (x, eyes_y), (x + w, eyes_y), (0, 0, 0), thickness=2)\n",
    "\n",
    "    # Show the final frame\n",
    "    cv2.imshow('Video with Silhouette', frame)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Initialize MediaPipe Pose and drawing utilities\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Function to calculate the angle between two vectors\n",
    "\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    # Points are given as (x, y) - Calculate the angle at point b\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "\n",
    "    # Vector BA and BC\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "\n",
    "    # Calculate the cosine of the angle using the dot product formula\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Function to transform points to simulate a side view (perpendicular transformation)\n",
    "\n",
    "\n",
    "def simulate_perpendicular_view(landmarks, width, height, side=\"left\"):\n",
    "    transformed_points = []\n",
    "\n",
    "    # Side view: all x-coordinates should be set to the same value for a perpendicular effect\n",
    "    shear_factor = 0.3 if side == \"left\" else -0.3\n",
    "    # Reference x-coordinate to align all points for the perpendicular view\n",
    "    ref_x = int(width / 2)\n",
    "\n",
    "    for landmark in landmarks:\n",
    "        # Convert normalized coordinates to pixel coordinates\n",
    "        x = int(landmark.x * width)\n",
    "        y = int(landmark.y * height)\n",
    "\n",
    "        # Apply a shear transformation for a basic simulation of side view, and set the x to ref_x\n",
    "        # Adjust x slightly to add a depth effect for better visualization\n",
    "        new_x = ref_x + int(shear_factor * y)\n",
    "        new_y = y\n",
    "\n",
    "        transformed_points.append((new_x, new_y))\n",
    "\n",
    "    return transformed_points\n",
    "\n",
    "\n",
    "# Load the input video (change 'myVideo.mp4' to your video path or use 0 for webcam)\n",
    "cap = cv2.VideoCapture('myVideo.mp4')\n",
    "\n",
    "# Get the video frame dimensions\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define the codec and create a VideoWriter object to save the output video\n",
    "out = cv2.VideoWriter('output_simulated_perpendicular_views.mp4',\n",
    "                      cv2.VideoWriter_fourcc(*'mp4v'), fps, (3 * width, height))\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video or failed to capture frame\")\n",
    "            break\n",
    "\n",
    "        # Convert the image to RGB for MediaPipe processing\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the image to detect pose landmarks\n",
    "        results = pose.process(image_rgb)\n",
    "\n",
    "        # Create a blank image for left and right side views\n",
    "        left_view = np.zeros_like(frame)\n",
    "        right_view = np.zeros_like(frame)\n",
    "\n",
    "        # Check if landmarks are detected\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Generate silhouette for the central view (overlayed on the real frame)\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            # Extract specific landmark positions for neck and shoulder calculations\n",
    "            NOSE_INDEX = 0\n",
    "            LEFT_SHOULDER_INDEX = 11\n",
    "            RIGHT_SHOULDER_INDEX = 12\n",
    "\n",
    "            # Get landmark positions in pixels\n",
    "            nose = (int(landmarks[NOSE_INDEX].x * width),\n",
    "                    int(landmarks[NOSE_INDEX].y * height))\n",
    "            left_shoulder = (int(landmarks[LEFT_SHOULDER_INDEX].x * width),\n",
    "                             int(landmarks[LEFT_SHOULDER_INDEX].y * height))\n",
    "            right_shoulder = (int(landmarks[RIGHT_SHOULDER_INDEX].x * width),\n",
    "                              int(landmarks[RIGHT_SHOULDER_INDEX].y * height))\n",
    "\n",
    "            # Draw neckline from nose to midpoint between shoulders\n",
    "            neck_midpoint = (\n",
    "                (left_shoulder[0] + right_shoulder[0]) // 2, (left_shoulder[1] + right_shoulder[1]) // 2)\n",
    "            cv2.line(frame, nose, neck_midpoint, (0, 0, 255), thickness=2)\n",
    "\n",
    "            # Draw shoulder line between left and right shoulder\n",
    "            cv2.line(frame, left_shoulder, right_shoulder,\n",
    "                     (0, 255, 0), thickness=2)\n",
    "\n",
    "            # Calculate and display the angle formed by the neck and shoulder line\n",
    "            neck_angle = calculate_angle(\n",
    "                left_shoulder, neck_midpoint, right_shoulder)\n",
    "            cv2.putText(frame, f'Angle: {int(neck_angle)} deg', (neck_midpoint[0] + 10, neck_midpoint[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "\n",
    "            # Simulate left and right side views using transformations\n",
    "            left_side_points = simulate_perpendicular_view(\n",
    "                landmarks, width, height, side=\"left\")\n",
    "            right_side_points = simulate_perpendicular_view(\n",
    "                landmarks, width, height, side=\"right\")\n",
    "\n",
    "            # Draw left side silhouette\n",
    "            for connection in mp_pose.POSE_CONNECTIONS:\n",
    "                start_idx = connection[0]\n",
    "                end_idx = connection[1]\n",
    "\n",
    "                # In a side view, lines like shoulders should be a point\n",
    "                # Shoulder and hip lines\n",
    "                if (start_idx, end_idx) in [(11, 12), (23, 24)]:\n",
    "                    start_coords = left_side_points[start_idx]\n",
    "                    cv2.circle(left_view, start_coords, radius=5,\n",
    "                               color=(255, 255, 255), thickness=-1)\n",
    "                else:\n",
    "                    start_coords = left_side_points[start_idx]\n",
    "                    end_coords = left_side_points[end_idx]\n",
    "                    cv2.line(left_view, start_coords,\n",
    "                             end_coords, (255, 255, 255), 5)\n",
    "\n",
    "            # Draw right side silhouette\n",
    "            for connection in mp_pose.POSE_CONNECTIONS:\n",
    "                start_idx = connection[0]\n",
    "                end_idx = connection[1]\n",
    "\n",
    "                # In a side view, lines like shoulders should be a point\n",
    "                # Shoulder and hip lines\n",
    "                if (start_idx, end_idx) in [(11, 12), (23, 24)]:\n",
    "                    start_coords = right_side_points[start_idx]\n",
    "                    cv2.circle(right_view, start_coords, radius=5,\n",
    "                               color=(255, 255, 255), thickness=-1)\n",
    "                else:\n",
    "                    start_coords = right_side_points[start_idx]\n",
    "                    end_coords = right_side_points[end_idx]\n",
    "                    cv2.line(right_view, start_coords,\n",
    "                             end_coords, (255, 255, 255), 5)\n",
    "\n",
    "        # Concatenate central, left, and right views\n",
    "        combined_view = np.hstack((left_view, frame, right_view))\n",
    "\n",
    "        # Show the combined view\n",
    "        cv2.imshow('Simulated Perpendicular Views', combined_view)\n",
    "\n",
    "        # Write the combined view to the output video\n",
    "        out.write(combined_view)\n",
    "\n",
    "        # Exit loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Initialize MediaPipe Pose and drawing utilities\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Function to calculate the angle between two vectors\n",
    "\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Function to transform points to simulate a side view (perpendicular transformation)\n",
    "\n",
    "\n",
    "def simulate_perpendicular_view(landmarks, width, height, side=\"left\"):\n",
    "    transformed_points = []\n",
    "    shear_factor = 0.3 if side == \"left\" else -0.3\n",
    "    ref_x = int(width / 2)\n",
    "\n",
    "    for landmark in landmarks:\n",
    "        x = int(landmark.x * width)\n",
    "        y = int(landmark.y * height)\n",
    "        new_x = ref_x + int(shear_factor * y)\n",
    "        new_y = y\n",
    "        transformed_points.append((new_x, new_y))\n",
    "\n",
    "    return transformed_points\n",
    "\n",
    "\n",
    "# Load the input video (change 'myVideo.mp4' to your video path or use 0 for webcam)\n",
    "cap = cv2.VideoCapture('myVideo.mp4')\n",
    "\n",
    "# Get the video frame dimensions\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define the codec and create a VideoWriter object to save the output video at 65% of original size\n",
    "output_width = int(3 * width * 0.65)\n",
    "output_height = int(height * 0.65)\n",
    "out = cv2.VideoWriter('output_simulated_perpendicular_views.mp4',\n",
    "                      cv2.VideoWriter_fourcc(*'mp4v'), fps, (output_width, output_height))\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video or failed to capture frame\")\n",
    "            break\n",
    "\n",
    "        # Convert the image to RGB for MediaPipe processing\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the image to detect pose landmarks\n",
    "        results = pose.process(image_rgb)\n",
    "\n",
    "        # Create a blank image for left and right side views\n",
    "        left_view = np.zeros_like(frame)\n",
    "        right_view = np.zeros_like(frame)\n",
    "\n",
    "        # Check if landmarks are detected\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Generate silhouette for the central view (overlayed on the real frame)\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            # Extract specific landmark positions for neck and shoulder calculations\n",
    "            NOSE_INDEX = 0\n",
    "            LEFT_SHOULDER_INDEX = 11\n",
    "            RIGHT_SHOULDER_INDEX = 12\n",
    "\n",
    "            # Get landmark positions in pixels\n",
    "            nose = (int(landmarks[NOSE_INDEX].x * width),\n",
    "                    int(landmarks[NOSE_INDEX].y * height))\n",
    "            left_shoulder = (int(landmarks[LEFT_SHOULDER_INDEX].x * width),\n",
    "                             int(landmarks[LEFT_SHOULDER_INDEX].y * height))\n",
    "            right_shoulder = (int(landmarks[RIGHT_SHOULDER_INDEX].x * width),\n",
    "                              int(landmarks[RIGHT_SHOULDER_INDEX].y * height))\n",
    "\n",
    "            # Draw neckline from nose to midpoint between shoulders\n",
    "            neck_midpoint = (\n",
    "                (left_shoulder[0] + right_shoulder[0]) // 2, (left_shoulder[1] + right_shoulder[1]) // 2)\n",
    "            cv2.line(frame, nose, neck_midpoint, (0, 0, 255), thickness=2)\n",
    "\n",
    "            # Draw shoulder line between left and right shoulder\n",
    "            cv2.line(frame, left_shoulder, right_shoulder,\n",
    "                     (0, 255, 0), thickness=2)\n",
    "\n",
    "            # Calculate and display the angle formed by the neck and shoulder line\n",
    "            neck_angle = calculate_angle(\n",
    "                left_shoulder, neck_midpoint, right_shoulder)\n",
    "            cv2.putText(frame, f'Angle: {int(neck_angle)} deg', (neck_midpoint[0] + 10, neck_midpoint[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "\n",
    "            # Simulate left and right side views using transformations\n",
    "            left_side_points = simulate_perpendicular_view(\n",
    "                landmarks, width, height, side=\"left\")\n",
    "            right_side_points = simulate_perpendicular_view(\n",
    "                landmarks, width, height, side=\"right\")\n",
    "\n",
    "            # Draw left and right side views including the neckline movement\n",
    "            for connection in mp_pose.POSE_CONNECTIONS:\n",
    "                start_idx = connection[0]\n",
    "                end_idx = connection[1]\n",
    "\n",
    "                start_coords_left = left_side_points[start_idx]\n",
    "                end_coords_left = left_side_points[end_idx]\n",
    "                start_coords_right = right_side_points[start_idx]\n",
    "                end_coords_right = right_side_points[end_idx]\n",
    "\n",
    "                # Adjust neckline based on head movement for lateral views\n",
    "                if start_idx == NOSE_INDEX and (end_idx == LEFT_SHOULDER_INDEX or end_idx == RIGHT_SHOULDER_INDEX):\n",
    "                    neck_offset = (\n",
    "                        neck_midpoint[0] - nose[0]) if end_idx == LEFT_SHOULDER_INDEX else -(neck_midpoint[0] - nose[0])\n",
    "                    new_neck_x_left = left_side_points[NOSE_INDEX][0] + \\\n",
    "                        neck_offset\n",
    "                    new_neck_x_right = right_side_points[NOSE_INDEX][0] - neck_offset\n",
    "\n",
    "                    # Draw the neck in the lateral views with appropriate shift\n",
    "                    cv2.line(left_view, (new_neck_x_left,\n",
    "                             neck_midpoint[1]), left_side_points[NOSE_INDEX], (255, 255, 255), thickness=2)\n",
    "                    cv2.line(right_view, (new_neck_x_right,\n",
    "                             neck_midpoint[1]), right_side_points[NOSE_INDEX], (255, 255, 255), thickness=2)\n",
    "                else:\n",
    "                    # For normal connections, draw the silhouette lines\n",
    "                    cv2.line(left_view, start_coords_left,\n",
    "                             end_coords_left, (255, 255, 255), 5)\n",
    "                    cv2.line(right_view, start_coords_right,\n",
    "                             end_coords_right, (255, 255, 255), 5)\n",
    "\n",
    "        # Concatenate central, left, and right views\n",
    "        combined_view = np.hstack((left_view, frame, right_view))\n",
    "\n",
    "        # Resize combined view to 65% of original size\n",
    "        combined_view_resized = cv2.resize(\n",
    "            combined_view, (output_width, output_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Show the combined view\n",
    "        cv2.imshow('Simulated Perpendicular Views', combined_view_resized)\n",
    "\n",
    "        # Write the resized combined view to the output video\n",
    "        out.write(combined_view_resized)\n",
    "\n",
    "        # Exit loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Pose and drawing utilities\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Function to calculate the midpoint between two points\n",
    "\n",
    "\n",
    "def calculate_midpoint(point1, point2):\n",
    "    return ((point1[0] + point2[0]) // 2, (point1[1] + point2[1]) // 2)\n",
    "\n",
    "# Function to simulate side view points by rotating landmarks\n",
    "\n",
    "\n",
    "def rotate_landmarks(landmarks, width, height, angle_degrees):\n",
    "    rotated_points = []\n",
    "    angle_radians = np.deg2rad(angle_degrees)\n",
    "    for landmark in landmarks:\n",
    "        x = landmark.x * width\n",
    "        y = landmark.y * height\n",
    "        rotated_x = x * np.cos(angle_radians) - y * np.sin(angle_radians)\n",
    "        rotated_y = x * np.sin(angle_radians) + y * np.cos(angle_radians)\n",
    "        rotated_points.append((int(rotated_x), int(rotated_y)))\n",
    "    return rotated_points\n",
    "\n",
    "# Function to add neck and face lines to the lateral views\n",
    "\n",
    "\n",
    "def add_vertical_lines(view, neck_midpoint, nose_central, flip):\n",
    "    # Add neck line\n",
    "    neck_line_end = (neck_midpoint[0], neck_midpoint[1] -\n",
    "                     (neck_midpoint[1] - nose_central[1]) * flip)\n",
    "    cv2.line(view, neck_midpoint, neck_line_end, (255, 255, 255), 5)\n",
    "    # Add face line\n",
    "    face_line_end = (nose_central[0], neck_midpoint[1] -\n",
    "                     (neck_midpoint[1] - nose_central[1]) * flip)\n",
    "    cv2.line(view, nose_central, face_line_end, (255, 0, 0), 5)\n",
    "\n",
    "# Function to add facial features to lateral views\n",
    "\n",
    "\n",
    "def add_facial_features(view, side_points):\n",
    "    left_eye = side_points[mp_pose.PoseLandmark.LEFT_EYE.value]\n",
    "    right_eye = side_points[mp_pose.PoseLandmark.RIGHT_EYE.value]\n",
    "    mouth_left = side_points[mp_pose.PoseLandmark.MOUTH_LEFT.value]\n",
    "    mouth_right = side_points[mp_pose.PoseLandmark.MOUTH_RIGHT.value]\n",
    "\n",
    "    # Drawing eyes\n",
    "    cv2.circle(view, left_eye, 5, (255, 255, 255), -1)\n",
    "    cv2.circle(view, right_eye, 5, (255, 255, 255), -1)\n",
    "\n",
    "    # Drawing mouth\n",
    "    cv2.line(view, mouth_left, mouth_right, (255, 255, 255), 5)\n",
    "\n",
    "\n",
    "# Load the input video (change 'myVideo.mp4' to your video path or use 0 for webcam)\n",
    "cap = cv2.VideoCapture('myVideo.mp4')\n",
    "\n",
    "# Get the video frame dimensions\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Resize dimensions to 65%\n",
    "resize_width = int(width * 0.65)\n",
    "resize_height = int(height * 0.65)\n",
    "\n",
    "# Define the codec and create a VideoWriter object to save the output video\n",
    "out = cv2.VideoWriter('output_simulated_side_views.mp4',\n",
    "                      cv2.VideoWriter_fourcc(*'mp4v'), fps, (3 * resize_width, resize_height))\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video or failed to capture frame\")\n",
    "            break\n",
    "\n",
    "        frame = cv2.resize(frame, (resize_width, resize_height))\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image_rgb)\n",
    "        left_view = np.zeros_like(frame)\n",
    "        right_view = np.zeros_like(frame)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            left_shoulder_central = (int(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x * resize_width),\n",
    "                                     int(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y * resize_height))\n",
    "            right_shoulder_central = (int(landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x * resize_width),\n",
    "                                      int(landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y * resize_height))\n",
    "            neck_midpoint_central = calculate_midpoint(\n",
    "                left_shoulder_central, right_shoulder_central)\n",
    "            nose_central = (int(landmarks[mp_pose.PoseLandmark.NOSE.value].x * resize_width),\n",
    "                            int(landmarks[mp_pose.PoseLandmark.NOSE.value].y * resize_height))\n",
    "\n",
    "            # Draw the vertical neck line behind the head\n",
    "            cv2.line(frame, neck_midpoint_central,\n",
    "                     nose_central, (255, 255, 255), 5)\n",
    "\n",
    "            left_side_points = rotate_landmarks(\n",
    "                landmarks, resize_width, resize_height, 90)\n",
    "            right_side_points = rotate_landmarks(\n",
    "                landmarks, resize_width, resize_height, -90)\n",
    "\n",
    "            for connection in mp_pose.POSE_CONNECTIONS:\n",
    "                start_idx = connection[0]\n",
    "                end_idx = connection[1]\n",
    "                start_coords = left_side_points[start_idx]\n",
    "                end_coords = left_side_points[end_idx]\n",
    "                cv2.line(left_view, start_coords,\n",
    "                         end_coords, (255, 255, 255), 5)\n",
    "\n",
    "                start_coords = right_side_points[start_idx]\n",
    "                end_coords = right_side_points[end_idx]\n",
    "                cv2.line(right_view, start_coords,\n",
    "                         end_coords, (255, 255, 255), 5)\n",
    "\n",
    "            # Add facial features to the lateral views\n",
    "            add_facial_features(left_view, left_side_points)\n",
    "            add_facial_features(right_view, right_side_points)\n",
    "\n",
    "            # Add neck and face lines to the lateral views with correct inclination\n",
    "            add_vertical_lines(\n",
    "                left_view, neck_midpoint_central, nose_central, 1)\n",
    "            add_vertical_lines(\n",
    "                right_view, neck_midpoint_central, nose_central, -1)\n",
    "\n",
    "        combined_view = np.hstack((left_view, frame, right_view))\n",
    "        cv2.imshow('Simulated Side Views', combined_view)\n",
    "        out.write(combined_view)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model to c:\\Users\\stefa\\anaconda3\\envs\\pyimage\\Lib\\site-packages\\mediapipe/modules/pose_landmark/pose_landmark_heavy.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stefa\\anaconda3\\envs\\pyimage\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. The output video is saved as: output_with_pose_estimation.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=2,\n",
    "                    enable_segmentation=False, min_detection_confidence=0.5)\n",
    "\n",
    "# Function to process each frame and estimate the 2D keypoints\n",
    "\n",
    "\n",
    "def process_frame(frame):\n",
    "    # Convert the frame to RGB for MediaPipe processing\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    # Extract 2D keypoints if available\n",
    "    keypoints = []\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            keypoints.append(\n",
    "                (int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0])))\n",
    "    return keypoints\n",
    "\n",
    "# Optional: Function to optimize and estimate 3D joint angles\n",
    "\n",
    "\n",
    "def optimize_3d_pose(keypoints_2d):\n",
    "    # Placeholder optimization code (you would use a 3D model here)\n",
    "    def objective_function(params):\n",
    "        # This function would calculate the difference between the 2D projection of a 3D model\n",
    "        # and the detected 2D keypoints.\n",
    "        # `params` would represent the 3D joint angles or positions.\n",
    "        # For simplicity, let's assume a mockup function that returns a constant error.\n",
    "        return np.sum((params - np.array([1, 2, 3])) ** 2)\n",
    "\n",
    "    # Initial guess for optimization\n",
    "    initial_guess = np.zeros(3)\n",
    "\n",
    "    # Run optimization to minimize the objective function\n",
    "    result = minimize(objective_function, initial_guess, method='BFGS')\n",
    "    # Return optimized 3D parameters (joint angles or positions)\n",
    "    return result.x\n",
    "\n",
    "\n",
    "# Video processing\n",
    "input_video_path = 'myvideo.mp4'\n",
    "output_video_path = 'output_with_pose_estimation.mp4'\n",
    "\n",
    "# Open video capture and writer\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps,\n",
    "                      (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process frame to get 2D keypoints\n",
    "    keypoints = process_frame(frame)\n",
    "\n",
    "    # Draw keypoints and lines on the frame\n",
    "    if keypoints:\n",
    "        for x, y in keypoints:\n",
    "            # Green circles for keypoints\n",
    "            cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\n",
    "        # Draw lines between keypoints (simplified connections for visualization)\n",
    "        # Add more connections as needed\n",
    "        skeleton_connections = [(11, 12), (11, 23), (12, 24), (23, 24)]\n",
    "        for pt1, pt2 in skeleton_connections:\n",
    "            if pt1 < len(keypoints) and pt2 < len(keypoints):\n",
    "                # Blue lines for connections\n",
    "                cv2.line(frame, keypoints[pt1], keypoints[pt2], (255, 0, 0), 2)\n",
    "\n",
    "    # Optionally, optimize 3D pose estimation based on 2D keypoints\n",
    "    optimized_params = optimize_3d_pose(keypoints) if keypoints else None\n",
    "\n",
    "    # Write frame to output video\n",
    "    out.write(frame)\n",
    "\n",
    "    # Display the frame with overlay (optional)\n",
    "    cv2.imshow('Pose Estimation', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Processing complete. The output video is saved as:\", output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. The output video is saved as: output_with_silhouette_and_face_line.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=2,\n",
    "                    enable_segmentation=False, min_detection_confidence=0.5)\n",
    "\n",
    "# Function to process each frame and estimate the 2D keypoints\n",
    "\n",
    "\n",
    "def process_frame(frame):\n",
    "    # Convert the frame to RGB for MediaPipe processing\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    # Extract 2D keypoints if available\n",
    "    keypoints = []\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            keypoints.append(\n",
    "                (int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0])))\n",
    "    return keypoints\n",
    "\n",
    "# Function to calculate the angle between two points (e.g., eyes)\n",
    "\n",
    "\n",
    "def calculate_angle(point1, point2):\n",
    "    dx = point2[0] - point1[0]\n",
    "    dy = point2[1] - point1[1]\n",
    "    angle = math.degrees(math.atan2(dy, dx))\n",
    "    return angle\n",
    "\n",
    "# Function to draw the head silhouette (two blue and two red lines)\n",
    "\n",
    "\n",
    "def draw_head_silhouette(frame, keypoints, angle):\n",
    "    # Define the center of the face and line lengths\n",
    "    LEFT_EYE_INDEX = 1  # Adjust index based on landmark layout\n",
    "    RIGHT_EYE_INDEX = 2  # Adjust index based on landmark layout\n",
    "\n",
    "    if len(keypoints) > max(LEFT_EYE_INDEX, RIGHT_EYE_INDEX):\n",
    "        # Get the coordinates of the eyes\n",
    "        eye_left = keypoints[LEFT_EYE_INDEX]\n",
    "        eye_right = keypoints[RIGHT_EYE_INDEX]\n",
    "\n",
    "        # Calculate the center between the eyes\n",
    "        face_center_x = (eye_left[0] + eye_right[0]) // 2\n",
    "        face_center_y = (eye_left[1] + eye_right[1]) // 2\n",
    "        head_line_length = 50\n",
    "        neck_line_length = 40\n",
    "\n",
    "        # Calculate angles for the two blue lines (head sides)\n",
    "        left_angle = angle - 30\n",
    "        right_angle = angle + 30\n",
    "\n",
    "        # Draw left blue line\n",
    "        left_end_x = int(face_center_x + head_line_length *\n",
    "                         math.cos(math.radians(left_angle)))\n",
    "        left_end_y = int(face_center_y + head_line_length *\n",
    "                         math.sin(math.radians(left_angle)))\n",
    "        cv2.line(frame, (face_center_x, face_center_y),\n",
    "                 (left_end_x, left_end_y), (255, 0, 0), 2)\n",
    "\n",
    "        # Draw right blue line\n",
    "        right_end_x = int(face_center_x + head_line_length *\n",
    "                          math.cos(math.radians(right_angle)))\n",
    "        right_end_y = int(face_center_y + head_line_length *\n",
    "                          math.sin(math.radians(right_angle)))\n",
    "        cv2.line(frame, (face_center_x, face_center_y),\n",
    "                 (right_end_x, right_end_y), (255, 0, 0), 2)\n",
    "\n",
    "        # Draw vertical red lines (neck) below each blue line end\n",
    "        # Left red line (neck)\n",
    "        neck_left_end_y = left_end_y + neck_line_length\n",
    "        cv2.line(frame, (left_end_x, left_end_y),\n",
    "                 (left_end_x, neck_left_end_y), (0, 0, 255), 2)\n",
    "\n",
    "        # Right red line (neck)\n",
    "        neck_right_end_y = right_end_y + neck_line_length\n",
    "        cv2.line(frame, (right_end_x, right_end_y),\n",
    "                 (right_end_x, neck_right_end_y), (0, 0, 255), 2)\n",
    "\n",
    "# Function to draw the vertical line on the face\n",
    "\n",
    "\n",
    "def draw_face_vertical_line(frame, keypoints, angle):\n",
    "    LEFT_EYE_INDEX = 1\n",
    "    RIGHT_EYE_INDEX = 2\n",
    "\n",
    "    if len(keypoints) > max(LEFT_EYE_INDEX, RIGHT_EYE_INDEX):\n",
    "        # Get the coordinates of the eyes\n",
    "        eye_left = keypoints[LEFT_EYE_INDEX]\n",
    "        eye_right = keypoints[RIGHT_EYE_INDEX]\n",
    "\n",
    "        # Determine the center point between the eyes\n",
    "        face_center_x = (eye_left[0] + eye_right[0]) // 2\n",
    "        face_center_y = (eye_left[1] + eye_right[1]) // 2\n",
    "\n",
    "        # Length of the vertical line\n",
    "        line_length = 50\n",
    "\n",
    "        # Calculate start and end points for the line based on inclination\n",
    "        line_end_x = int(face_center_x + line_length *\n",
    "                         math.cos(math.radians(angle)))\n",
    "        line_end_y = int(face_center_y + line_length *\n",
    "                         math.sin(math.radians(angle)))\n",
    "        line_start_x = int(face_center_x - line_length *\n",
    "                           math.cos(math.radians(angle)))\n",
    "        line_start_y = int(face_center_y - line_length *\n",
    "                           math.sin(math.radians(angle)))\n",
    "\n",
    "        # Draw the vertical face line\n",
    "        cv2.line(frame, (line_start_x, line_start_y), (line_end_x,\n",
    "                 line_end_y), (0, 255, 0), 2)  # Green color for face line\n",
    "\n",
    "\n",
    "# Video processing\n",
    "input_video_path = 'myvideo.mp4'\n",
    "output_video_path = 'output_with_silhouette_and_face_line.mp4'\n",
    "\n",
    "# Open video capture and writer\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps,\n",
    "                      (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process frame to get 2D keypoints\n",
    "    keypoints = process_frame(frame)\n",
    "\n",
    "    # Check if we have both eyes to calculate the inclination angle\n",
    "    if len(keypoints) >= 3:\n",
    "        # Get eye positions and calculate head tilt angle\n",
    "        eye_left = keypoints[1]\n",
    "        eye_right = keypoints[2]\n",
    "        angle = calculate_angle(eye_left, eye_right)\n",
    "\n",
    "        # Draw the main head silhouette (blue and red lines)\n",
    "        draw_head_silhouette(frame, keypoints, angle)\n",
    "\n",
    "        # Draw the vertical line on the face\n",
    "        draw_face_vertical_line(frame, keypoints, angle)\n",
    "\n",
    "    # Write frame to output video\n",
    "    out.write(frame)\n",
    "\n",
    "    # Display the frame with overlay (optional)\n",
    "    cv2.imshow('Silhouette with Face Line', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Processing complete. The output video is saved as:\", output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyimage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
