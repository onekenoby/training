{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Neo4j is a highly scalable, native graph database that is designed to store, manage, and query highly connected data. It is a popular choice for applications that require complex data modeling and analysis, such as social networks, recommendation engines, fraud detection, and network and IT operations. Neo4j uses a property graph data model, which allows for the representation of relationships between data points in addition to the data itself. It also supports a powerful query language called Cypher, which makes it easy to traverse and manipulate graph data. Neo4j is used by organizations of all sizes, from startups to Fortune 500 companies, and is known for its fast performance, flexibility, and ease of use.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(\n",
    "    openai_api_key=\"sk-...\",\n",
    "    model=\"gpt-3.5-turbo-instruct\", #babbage-002\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Neo4j is a graph database management system that is designed to store, manage, and query highly connected data. It uses a graph data model, where data is represented as nodes (entities) and relationships (connections between nodes). This allows for efficient storage and retrieval of complex and interconnected data. Neo4j is often used for applications that require real-time querying and analysis of large and highly connected datasets, such as social networks, recommendation engines, and fraud detection systems. It is also commonly used in fields such as bioinformatics, supply chain management, and network and IT operations.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0\n",
    ")\n",
    "response = llm.invoke(\"What is Neo4j?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "Python is a high-level, interpreted, object-oriented programming language. It was created by Guido van Rossum in 1991 and has since become one of the most popular programming languages in the world. Python is known for its simple and easy-to-learn syntax, which makes it a great language for beginners. It is also widely used in various fields such as web development, data science, machine learning, and scientific computing. Python has a large and active community that contributes to its growth and development, making it a versatile and powerful language for a wide range of applications. \n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"What is Python\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Well, apples and pears, me old china plate! That's a proper tasty fruit, mate. Can't go wrong with an apple a day, keeps the doctor away, they say. Crisp and juicy, it's the bee's knees. Pick some up and you'll be pleased!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(template=\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\", \n",
    "input_variables=[\"fruit\"])\n",
    "response = llm.invoke(template.format(fruit=\"apple\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Well, mate, that's a right juicy Adam and Eve you got there. Perfect for a nice pie or crumble, innit?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(openai_api_key=\"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\",\n",
    "             model=\"gpt-3.5-turbo-instruct\", #model=\"gpt-3.5-turbo-instruct\", \n",
    "            temperature=0\n",
    ")\n",
    "\n",
    "template = PromptTemplate(template=\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\", input_variables=[\"fruit\"])\n",
    "\n",
    "response = llm.invoke(template.format(fruit=\"apple\"))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Well, mate, that's a right juicy Adam and Eve you got there. Perfect for a nice pie or crumble, innit?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0\n",
    ")\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\")\n",
    "\n",
    "llm_chain = template | llm\n",
    "\n",
    "response = llm_chain.invoke({\"fruit\": \"apple\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\"description\": \"Apples and pears, mate? They're the bee's knees! Crunchy and sweet, they'll make your taste buds sing.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "llm = OpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0\n",
    ")\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Output JSON as {{\"description\": \"your response here\"}}\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\")\n",
    "\n",
    "llm_chain = template | llm | StrOutputParser()\n",
    "\n",
    "response = llm_chain.invoke({\"fruit\": \"apple\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Surfer: Dude, the weather is totally gnarly today! The waves are pumping and the sun is shining. It's gonna be a sick sesh out there.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  \n",
    "\n",
    "chat_llm = OpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0\n",
    ")\n",
    "\n",
    "#system message\n",
    "instructions = SystemMessage(content=\"\"\"\n",
    "You are a surfer dude, having a conversation about the surf conditions on the beach.\n",
    "Respond using surfer slang.\n",
    "\"\"\")\n",
    "\n",
    "#human message \n",
    "question = HumanMessage(content=\"What is the weather like?\")\n",
    "\n",
    "\n",
    "response = chat_llm.invoke([\n",
    "    instructions,\n",
    "    question\n",
    "])\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "System: Dude, the weather is totally gnarly today! The waves are totally firing and the wind is offshore. It's gonna be a sick sesh out there.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The prompt is created by combining system and human messages.\n",
    "The chain is created using the chat model, the prompt and an output parser.\n",
    "The question is passed to the chat model as a parameter of the invoke method.\n",
    "--> LCEL (LangChain Expression Language)\n",
    "\"\"\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "chat_llm = OpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\", \n",
    "            \"{question}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#LCEL (LangChain Expression Language)\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "response = chat_chain.invoke({\"question\": \"What is the weather like?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "chat_llm = OpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        ( \"system\", \"{context}\" ),\n",
    "        ( \"human\", \"{question}\" ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "current_weather = \"\"\"\n",
    "    {\n",
    "        \"surf\": [\n",
    "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
    "            {\"beach\": \"Polzeath\", \"conditions\": \"Flat and calm\"},\n",
    "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
    "        ]\n",
    "    }\"\"\"\n",
    "\n",
    "response = chat_chain.invoke(\n",
    "    {\n",
    "        \"context\": current_weather,\n",
    "        \"question\": \"What is the weather like on Watergate Bay?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System: Dude, the conditions are gnarly! There's some sick 3ft waves, but the winds are totally onshore. It's gonna be a challenge out there.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "chat_llm = OpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        ( \"system\", \"{context}\" ),\n",
    "        ( \"human\", \"{question}\" ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "current_weather = \"\"\"\n",
    "    {\n",
    "        \"surf\": [\n",
    "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
    "            {\"beach\": \"Polzeath\", \"conditions\": \"Flat and calm\"},\n",
    "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
    "        ]\n",
    "    }\"\"\"\n",
    "\n",
    "response = chat_chain.invoke(\n",
    "    {\n",
    "        \"context\": current_weather,\n",
    "        \"question\": \"What is the weather like on Watergate Bay?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System: Le condizioni sono abbastanza buone, ci sono onde alte 3 piedi e venti da terra. Potrebbe essere un buon spot per fare surf oggi.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "chat_llm = OpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Sei un surfista e stai parlando delle condizioni del surf in spiaggia. Rispondi usando lo slang dei surfisti.\",\n",
    "        ),\n",
    "        ( \"system\", \"{context}\" ),\n",
    "        ( \"human\", \"{question}\" ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "current_weather = \"\"\"\n",
    "    {\n",
    "        \"surf\": [\n",
    "            {\"beach\": \"Fistral\", \"conditions\": \"Onde alte 6 piedi e venti offshore\"},\n",
    "            {\"beach\": \"Polzeath\", \"conditions\": \"Piatto e calmo\"},\n",
    "            {\"beach\": \"Watergate Bay\", \"conditions\": \"Onde alte 3 piedi e venti da terra\"}\n",
    "        ]\n",
    "    }\"\"\"\n",
    "\n",
    "response = chat_chain.invoke(\n",
    "    {\n",
    "        \"context\": current_weather,\n",
    "        \"question\": \"Com'è il meteo a Watergate Bay?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Conversation Memory:  Chat Model Memory\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "chat_llm = OpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        (\"system\", \"{context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), ### <- to mamorize the chat\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "current_weather = \"\"\"\n",
    "    {\n",
    "        \"surf\": [\n",
    "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
    "            {\"beach\": \"Polzeath\", \"conditions\": \"Flat and calm\"},\n",
    "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
    "        ]\n",
    "    }\"\"\"\n",
    "\n",
    "response = chat_chain.invoke(\n",
    "    {\n",
    "        \"context\": current_weather,\n",
    "        \"question\": \"What is the weather like on Watergate Bay?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System: Yo dude, the surf at Watergate Bay is looking pretty gnarly today. We got some solid 3ft waves rolling in, but watch out for those onshore winds. Might be a bit choppy out there. How's it looking at Fistral?\n",
      " It's totally flat and calm.\n",
      "AI: Bummer, man. Sounds like Polzeath is not the spot to be today. Maybe we should head over to Fistral and catch some sick 6ft waves with those offshore winds. What do you think?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Conversation Memory:  Chat Model Memory Wrapping\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "chat_llm = OpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "memory = ChatMessageHistory()\n",
    "\n",
    "def get_memory(session_id):\n",
    "    return memory\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        (\"system\", \"{context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), ### <- to mamorize the chat\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "chat_with_message_history = RunnableWithMessageHistory(\n",
    "    chat_chain,\n",
    "    get_memory,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "current_weather = \"\"\"\n",
    "    {\n",
    "        \"surf\": [\n",
    "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
    "            {\"beach\": \"Polzeath\", \"conditions\": \"Flat and calm\"},\n",
    "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
    "        ]\n",
    "    }\"\"\"\n",
    "\n",
    "\n",
    "response = chat_with_message_history.invoke(\n",
    "    {\n",
    "        \"context\": current_weather,\n",
    "        \"question\": \"Hi, I am at Watergate Bay. What is the surf like?\"\n",
    "    },\n",
    "    config={\"configurable\": {\"session_id\": \"none\"}}\n",
    ")\n",
    "print(response)\n",
    "\n",
    "response = chat_with_message_history.invoke(\n",
    "    {\n",
    "        \"context\": current_weather,\n",
    "        \"question\": \"Where I am?\"\n",
    "    },\n",
    "    config={\"configurable\": {\"session_id\": \"none\"}}\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chat_llm = ChatOpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        (\"system\", \"{context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ChatMessageHistory()\n",
    "\n",
    "def get_memory(session_id):\n",
    "    return memory\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "chat_with_message_history = RunnableWithMessageHistory(\n",
    "    chat_chain,\n",
    "    get_memory,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "\n",
    "current_weather = \"\"\"\n",
    "    {\n",
    "        \"surf\": [\n",
    "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
    "            {\"beach\": \"Bells\", \"conditions\": \"Flat and calm\"},\n",
    "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
    "        ]\n",
    "    }\"\"\"\n",
    "\n",
    "while True:\n",
    "    question = input(\"> \")\n",
    "\n",
    "    response = chat_with_message_history.invoke(\n",
    "        {\n",
    "            \"context\": current_weather,\n",
    "            \"question\": question,\n",
    "            \n",
    "        }, \n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": \"none\"}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'m.title': 'The Matrix', 'm.plot': None, 'm.poster': None}]\n",
      "\n",
      "\n",
      "\n",
      "Node properties:\n",
      "Person {name: STRING, born: INTEGER}\n",
      "Movie {released: INTEGER, tagline: STRING, title: STRING}\n",
      "Relationship properties:\n",
      "ACTED_IN {roles: LIST}\n",
      "REVIEWED {summary: STRING, rating: INTEGER}\n",
      "The relationships:\n",
      "(:Person)-[:ACTED_IN]->(:Movie)\n",
      "(:Person)-[:DIRECTED]->(:Movie)\n",
      "(:Person)-[:PRODUCED]->(:Movie)\n",
      "(:Person)-[:WROTE]->(:Movie)\n",
      "(:Person)-[:FOLLOWS]->(:Person)\n",
      "(:Person)-[:REVIEWED]->(:Movie)\n"
     ]
    }
   ],
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7476\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"onekenoby\"\n",
    ")\n",
    "\n",
    "result = graph.query(\"\"\"\n",
    "MATCH (m:Movie{title: 'The Matrix'}) \n",
    "RETURN m.title, m.plot, m.poster\n",
    "\"\"\")\n",
    "\n",
    "print(result)\n",
    "print(\"\\n\\n\")\n",
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey dude, I'm totally stoked! Have you checked out the waves at Fistral? It's firing with 6ft waves and offshore winds!\n",
      "No worries, dude! If you're keen to catch some sick waves, Fistral is where it's at right now. The conditions are epic with 6ft waves and offshore winds. It's gonna be gnarly out there!\n",
      "No problem, bro! Catch you later and hang loose out there on the waves!\n",
      "Catch you on the flip side, dude! Stay stoked and keep shredding those waves! Peace out!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chat_llm = ChatOpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        (\"system\", \"{context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ChatMessageHistory()\n",
    "\n",
    "def get_memory(session_id):\n",
    "    return memory\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "chat_with_message_history = RunnableWithMessageHistory(\n",
    "    chat_chain,\n",
    "    get_memory,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "\n",
    "current_weather = \"\"\"\n",
    "    {\n",
    "        \"surf\": [\n",
    "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
    "            {\"beach\": \"Bells\", \"conditions\": \"Flat and calm\"},\n",
    "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
    "        ]\n",
    "    }\"\"\"\n",
    "\n",
    "while True:\n",
    "    question = input(\"> \")\n",
    "\n",
    "    response = chat_with_message_history.invoke(\n",
    "        {\n",
    "            \"context\": current_weather,\n",
    "            \"question\": question,\n",
    "            \n",
    "        }, \n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": \"none\"}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: e2ab723f-2c0c-454e-95e9-b6bb2b41b404\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "SESSION_ID = str(uuid4())\n",
    "print(f\"Session ID: {SESSION_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jChatMessageHistory\n",
    "\n",
    "def get_memory(session_id):\n",
    "    return Neo4jChatMessageHistory(session_id=session_id, graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: e35fe227-28c4-4283-bb2f-34e6009ea777\n",
      "Dude, have you checked out the waves at Fistral? It's firing with 6ft sets and offshore winds! Bells is totally flat and calm though. Watergate Bay has some fun 3ft waves, but the winds are onshore. Where are you thinking of paddling out today?\n",
      "Hey, I'm totally stoked, dude! Just hanging loose and checking out the surf conditions. How about you, ready to catch some gnarly waves?\n",
      "The weather is looking pretty rad, dude! Clear skies and sunshine, perfect for hitting the beach and catching some waves. Just make sure to slap on some sunscreen and stay hydrated out there. It's gonna be a sick day for surfing!\n",
      "Right on, dude! Let's shred some waves and soak up those good vibes. Catch you out in the lineup!\n",
      "Catch you later, dude! Stay stoked and keep charging those waves!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain_neo4j import Neo4jChatMessageHistory\n",
    "from uuid import uuid4\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "SESSION_ID = str(uuid4())\n",
    "print(f\"Session ID: {SESSION_ID}\")\n",
    "\n",
    "chat_llm = ChatOpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0\n",
    ")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7476\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"onekenoby\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        (\"system\", \"{context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def get_memory(session_id):\n",
    "    return Neo4jChatMessageHistory(session_id=session_id, graph=graph)\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "chat_with_message_history = RunnableWithMessageHistory(\n",
    "    chat_chain,\n",
    "    get_memory,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "current_weather = \"\"\"\n",
    "    {\n",
    "        \"surf\": [\n",
    "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
    "            {\"beach\": \"Bells\", \"conditions\": \"Flat and calm\"},\n",
    "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
    "        ]\n",
    "    }\"\"\"\n",
    "\n",
    "while True:\n",
    "    question = input(\"> \")\n",
    "\n",
    "    response = chat_with_message_history.invoke(\n",
    "        {\n",
    "            \"context\": current_weather,\n",
    "            \"question\": question,\n",
    "            \n",
    "        }, \n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": SESSION_ID}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(response)\n",
    "    \n",
    "\n",
    "#conversation history in Neo4j\n",
    "\"\"\"\n",
    "MATCH (s:Session)-[:LAST_MESSAGE]->(last:Message)<-[:NEXT*]-(msg:Message)\n",
    "RETURN s, last, msg\n",
    "\n",
    "---\n",
    "\n",
    "MATCH (s:Session)-[:LAST_MESSAGE]->(last:Message)\n",
    "WHERE s.id = 'your session id'\n",
    "MATCH p = (last)<-[:NEXT*]-(msg:Message)\n",
    "UNWIND nodes(p) as msgs\n",
    "RETURN DISTINCT msgs.type, msgs.content\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: a6767219-b184-48d0-8fe1-1491f59c7f9b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stefa\\anaconda3\\envs\\pyimage\\Lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Arrival\" (2016) is a science fiction film where aliens land on Earth and a linguist is tasked with communicating with them to understand their purpose.\n",
      "You're welcome! If you have any more questions or need assistance, feel free to ask.\n",
      "Sì, posso parlare in italiano! Come posso aiutarti oggi?\n",
      "Un film con supereroi molto popolare è \"The Avengers\" del 2012, diretto da Joss Whedon. Il film vede l'unione di supereroi come Iron Man, Captain America, Thor, Hulk, Black Widow e Hawkeye per combattere insieme contro il malvagio Loki e il suo esercito alieno.\n",
      "Here are some popular superhero movies:\n",
      "\n",
      "1. \"The Avengers\" (2012) - A group of superheroes, including Iron Man, Captain America, Thor, Hulk, Black Widow, and Hawkeye, come together to fight an alien threat.\n",
      "\n",
      "2. \"The Dark Knight\" (2008) - Batman must face off against the criminal Joker while trying to protect Gotham City.\n",
      "\n",
      "3. \"Spider-Man: Into the Spider-Verse\" (2018) - Young Miles Morales discovers he has powers similar to Spider-Man and joins other Spider-People from alternate universes to save the city.\n",
      "\n",
      "4. \"Wonder Woman\" (2017) - Amazon princess Diana Prince becomes the superhero Wonder Woman and fights during World War I.\n",
      "\n",
      "5. \"Black Panther\" (2018) - T'Challa, the king of Wakanda, takes on the role of Black Panther and must defend his kingdom from internal and external threats.\n",
      "\n",
      "6. \"Guardians of the Galaxy\" (2014) - A group of space criminals, including Star-Lord, Gamora, Drax, Rocket, and Groot, come together to protect the universe from a cosmic threat.\n",
      "\n",
      "7. \"Deadpool\" (2016) - Mercenary Wade Wilson gains healing powers and becomes the sarcastic anti-hero Deadpool, on a mission of revenge.\n",
      "\n",
      "8. \"Captain Marvel\" (2019) - Carol Danvers, an Air Force pilot, becomes the powerful superhero Captain Marvel and uncovers the truth about her past.\n",
      "\n",
      "These are just a few examples of successful superhero movies that have resonated with audiences.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_neo4j import Neo4jChatMessageHistory, Neo4jGraph\n",
    "from uuid import uuid4\n",
    "\n",
    "SESSION_ID = str(uuid4())\n",
    "print(f\"Session ID: {SESSION_ID}\")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7476\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"onekenoby\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a movie expert. You find movies from a genre or plot.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_chat = prompt | llm | StrOutputParser()\n",
    "\n",
    "def get_memory(session_id):\n",
    "    return Neo4jChatMessageHistory(session_id=session_id, graph=graph)\n",
    "\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        name=\"Movie Chat\",\n",
    "        description=\"For when you need to chat about movies. The question will be a string. Return a string.\",\n",
    "        func=movie_chat.invoke,\n",
    "    )\n",
    "]\n",
    "\n",
    "agent_prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "agent = create_react_agent(llm, tools, agent_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "chat_agent = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    get_memory,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "while True:\n",
    "    q = input(\"> \")\n",
    "\n",
    "    response = chat_agent.invoke(\n",
    "        {\n",
    "            \"input\": q\n",
    "        },\n",
    "        {\"configurable\": {\"session_id\": SESSION_ID}},\n",
    "    )\n",
    "    \n",
    "    print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: 3b755457-cf26-4ebe-9d26-d067698bc6a3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stefa\\anaconda3\\envs\\pyimage\\Lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Movie Chat\n",
      "Action Input: Can you recommend a good sci-fi movie?\u001b[0m\u001b[36;1m\u001b[1;3mCertainly! One highly recommended sci-fi movie is \"Blade Runner 2049.\" This visually stunning film is a sequel to the original \"Blade Runner\" and follows a new blade runner, Officer K, as he uncovers a long-buried secret that has the potential to plunge what's left of society into chaos. With incredible visuals, a thought-provoking storyline, and outstanding performances, \"Blade Runner 2049\" is a must-watch for any sci-fi fan.\u001b[0m\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "Final Answer: Thank you for the recommendation! I'll be sure to check out \"Blade Runner 2049.\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Thank you for the recommendation! I'll be sure to check out \"Blade Runner 2049.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_neo4j import Neo4jChatMessageHistory, Neo4jGraph\n",
    "from uuid import uuid4\n",
    "\n",
    "SESSION_ID = str(uuid4())\n",
    "print(f\"Session ID: {SESSION_ID}\")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7476\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"onekenoby\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a movie expert. You find movies from a genre or plot.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_chat = prompt | llm | StrOutputParser()\n",
    "\n",
    "def get_memory(session_id):\n",
    "    return Neo4jChatMessageHistory(session_id=session_id, graph=graph)\n",
    "\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        name=\"Movie Chat\",\n",
    "        description=\"For when you need to chat about movies. The question will be a string. Return a string.\",\n",
    "        func=movie_chat.invoke,\n",
    "    )\n",
    "]\n",
    "\n",
    "agent_prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "agent = create_react_agent(llm, tools, agent_prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    max_iterations=3,\n",
    "    verbose=True,\n",
    "    handle_parse_errors=True\n",
    ")\n",
    "\n",
    "chat_agent = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    get_memory,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    q = input(\"> \")\n",
    "\n",
    "    response = chat_agent.invoke(\n",
    "        {\n",
    "            \"input\": q\n",
    "        },\n",
    "        {\"configurable\": {\"session_id\": SESSION_ID}},\n",
    "    )\n",
    "    \n",
    "    print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: 99cc6355-bab0-44df-a7ce-549cd924f8a7\n",
      "One movie that explores the meaning of life is \"The Tree of Life\" directed by Terrence Malick. This visually stunning film follows the journey of a man reflecting on his childhood and the complexities of existence, touching on themes of family, nature, and spirituality. Through its poetic storytelling and breathtaking cinematography, \"The Tree of Life\" delves deep into the philosophical question of the meaning of life.\n",
      "Here are two links to the trailer of \"The Tree of Life\":\n",
      "1. [Link 1](https://www.youtube.com/watch?v=RrAz1YLh8nY&pp=ygUQVGhlIFRyZWUgb2YgTGlmZQ%3D%3D)\n",
      "2. [Link 2](https://www.youtube.com/watch?v=xigx-FCnppM&pp=ygUQVGhlIFRyZWUgb2YgTGlmZQ%3D%3D)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_community.tools import YouTubeSearchTool\n",
    "from langchain_neo4j import Neo4jChatMessageHistory, Neo4jGraph\n",
    "from uuid import uuid4\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SESSION_ID = str(uuid4())\n",
    "print(f\"Session ID: {SESSION_ID}\")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=\"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\",\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7476\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"onekenoby\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a movie expert. You find movies from a genre or plot.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_chat = prompt | llm | StrOutputParser()\n",
    "\n",
    "youtube = YouTubeSearchTool()\n",
    "\n",
    "def get_memory(session_id):\n",
    "    return Neo4jChatMessageHistory(session_id=session_id, graph=graph)\n",
    "\n",
    "def call_trailer_search(input):\n",
    "    input = input.replace(\",\", \" \")\n",
    "    return youtube.run(input)\n",
    "\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        name=\"Movie Chat\",\n",
    "        description=\"For when you need to chat about movies. The question will be a string. Return a string.\",\n",
    "        func=movie_chat.invoke,\n",
    "    ),\n",
    "    Tool.from_function(\n",
    "        name=\"Movie Trailer Search\",\n",
    "        description=\"Use when needing to find a movie trailer. The question will include the word trailer. Return a link to a YouTube video.\",\n",
    "        func=call_trailer_search,\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent_prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "agent = create_react_agent(llm, tools, agent_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "chat_agent = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    get_memory,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "while True:\n",
    "    q = input(\"> \")\n",
    "\n",
    "    response = chat_agent.invoke(\n",
    "        {\n",
    "            \"input\": q\n",
    "        },\n",
    "        {\"configurable\": {\"session_id\": SESSION_ID}},\n",
    "    )\n",
    "    \n",
    "    print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coneheads - Aliens with conical crania crash land on Earth.\n",
      "Aliens - The planet from Alien (1979) has been colonized, but contact is lost. This time, the rescue team has impressive firepower, but will it be enough?\n",
      "Arrival, The - Zane, an astronomer, discovers intelligent alien life. But the aliens are keeping a deadly secret, and will do anything to stop Zane from learning it.\n",
      "Independence Day (a.k.a. ID4) - The aliens are coming and their goal is to invade and destroy Earth. Fighting superior technology, mankind's best weapon is the will to survive.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_neo4j import Neo4jGraph, Neo4jVector\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "embedding_provider = OpenAIEmbeddings(openai_api_key=\"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\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://3.93.212.184:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"clerks-circuitries-deployments\"\n",
    ")\n",
    "\n",
    "movie_plot_vector = Neo4jVector.from_existing_index(\n",
    "    embedding_provider,\n",
    "    graph=graph,\n",
    "    index_name=\"moviePlots\",\n",
    "    embedding_node_property=\"plotEmbedding\",\n",
    "    text_node_property=\"plot\",\n",
    ")\n",
    "\n",
    "result = movie_plot_vector.similarity_search(\"A movie where aliens land and attack earth.\")\n",
    "for doc in result:\n",
    "    print(doc.metadata[\"title\"], \"-\", doc.page_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_neo4j import Neo4jGraph, Neo4jVector\n",
    "from langchain.schema import Document\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# A list of Documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Text to be indexed\",\n",
    "        metadata={\"source\": \"local\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Service used to create the embeddings\n",
    "embedding_provider = OpenAIEmbeddings(openai_api_key=\"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\"\n",
    ")\n",
    "\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7476\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"onekenoby\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "new_vector = Neo4jVector.from_documents(\n",
    "    documents,\n",
    "    embedding_provider,\n",
    "    graph=graph,\n",
    "    index_name=\"myVectorIndex\",\n",
    "    node_label=\"Chunk\",\n",
    "    text_node_property=\"text\",\n",
    "    embedding_node_property=\"embedding\",\n",
    "    create_id_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_neo4j import Neo4jGraph, Neo4jVector\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "embedding_provider = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7476\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"onekenoby\"\n",
    ")\n",
    "\n",
    "movie_plot_vector = Neo4jVector.from_existing_index(\n",
    "    embedding_provider,\n",
    "    graph=graph,\n",
    "    index_name=\"moviePlots\",\n",
    "    embedding_node_property=\"plotEmbedding\",\n",
    "    text_node_property=\"plot\",\n",
    ")\n",
    "\n",
    "plot_retriever = RetrievalQA.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=movie_plot_vector.as_retriever()\n",
    ")\n",
    "\n",
    "response = plot_retriever.invoke(\n",
    "    {\"query\": \"A movie where a mission to the moon goes wrong\"}\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: ea9a8d6d-4703-4dac-9fb4-395015ea48d0\n",
      "\"Blade Runner 2049\" is a highly acclaimed science fiction movie that delves into themes of identity and artificial intelligence in a dystopian future. It is known for its stunning visuals and thought-provoking story.\n",
      "A classic romantic movie that comes to mind is \"The Notebook\" (2004). This film follows the love story between Noah and Allie, played by Ryan Gosling and Rachel McAdams. It's a heartwarming and emotional tale of love, loss, and second chances that has captured the hearts of audiences around the world.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_community.tools import YouTubeSearchTool\n",
    "from langchain_neo4j import Neo4jChatMessageHistory, Neo4jGraph\n",
    "from uuid import uuid4\n",
    "\n",
    "SESSION_ID = str(uuid4())\n",
    "print(f\"Session ID: {SESSION_ID}\")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=\"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\"\n",
    ")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7476\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"onekenoby\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a movie expert. You find movies from a genre or plot.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_chat = prompt | llm | StrOutputParser()\n",
    "\n",
    "youtube = YouTubeSearchTool()\n",
    "\n",
    "def get_memory(session_id):\n",
    "    return Neo4jChatMessageHistory(session_id=session_id, graph=graph)\n",
    "\n",
    "def call_trailer_search(input):\n",
    "    input = input.replace(\",\", \" \")\n",
    "    return youtube.run(input)\n",
    "\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        name=\"Movie Chat\",\n",
    "        description=\"For when you need to chat about movies. The question will be a string. Return a string.\",\n",
    "        func=movie_chat.invoke,\n",
    "    ),\n",
    "    Tool.from_function(\n",
    "        name=\"Movie Trailer Search\",\n",
    "        description=\"Use when needing to find a movie trailer. The question will include the word trailer. Return a link to a YouTube video.\",\n",
    "        func=call_trailer_search,\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent_prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "agent = create_react_agent(llm, tools, agent_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "chat_agent = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    get_memory,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "while True:\n",
    "    q = input(\"> \")\n",
    "\n",
    "    response = chat_agent.invoke(\n",
    "        {\n",
    "            \"input\": q\n",
    "        },\n",
    "        {\"configurable\": {\"session_id\": SESSION_ID}},\n",
    "    )\n",
    "    \n",
    "    print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: 3411d6c6-a618-4b1d-b4e2-d4cb0c577d73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stefa\\anaconda3\\envs\\pyimage\\Lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two links to the trailers for the movie \"Avatar\":\n",
      "1. [Avatar Trailer 1](https://www.youtube.com/watch?v=5PSNL1qE6VY&pp=ygUPQXZhdGFyIHRyYWlsZXIK)\n",
      "2. [Avatar Trailer 2](https://www.youtube.com/watch?v=MLp7-KB-xdk&pp=ygUPQXZhdGFyIHRyYWlsZXIK)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_community.tools import YouTubeSearchTool\n",
    "from langchain_neo4j import Neo4jChatMessageHistory, Neo4jGraph\n",
    "from uuid import uuid4\n",
    "\n",
    "SESSION_ID = str(uuid4())\n",
    "print(f\"Session ID: {SESSION_ID}\")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=\"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\"\n",
    ")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7476\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"onekenoby\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a movie expert. You find movies from a genre or plot.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_chat = prompt | llm | StrOutputParser()\n",
    "\n",
    "youtube = YouTubeSearchTool()\n",
    "\n",
    "def get_memory(session_id):\n",
    "    return Neo4jChatMessageHistory(session_id=session_id, graph=graph)\n",
    "\n",
    "def call_trailer_search(input):\n",
    "    input = input.replace(\",\", \" \")\n",
    "    return youtube.run(input)\n",
    "\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        name=\"Movie Chat\",\n",
    "        description=\"For when you need to chat about movies. The question will be a string. Return a string.\",\n",
    "        func=movie_chat.invoke,\n",
    "    ),\n",
    "    Tool.from_function(\n",
    "        name=\"Movie Trailer Search\",\n",
    "        description=\"Use when needing to find a movie trailer. The question will include the word trailer. Return a link to a YouTube video.\",\n",
    "        func=call_trailer_search,\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent_prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "agent = create_react_agent(llm, tools, agent_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "chat_agent = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    get_memory,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "while True:\n",
    "    q = input(\"> \")\n",
    "\n",
    "    response = chat_agent.invoke(\n",
    "        {\n",
    "            \"input\": q\n",
    "        },\n",
    "        {\"configurable\": {\"session_id\": SESSION_ID}},\n",
    "    )\n",
    "    \n",
    "    print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_neo4j import Neo4jGraph, Neo4jVector\n",
    "\n",
    "OPENAI_API_KEY = (\"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\"\n",
    ")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "embedding_provider = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7476\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"onekenoby\"\n",
    ")\n",
    "\n",
    "movie_plot_vector = Neo4jVector.from_existing_index(\n",
    "    embedding_provider,\n",
    "    graph=graph,\n",
    "    index_name=\"moviePlots\",\n",
    "    embedding_node_property=\"plotEmbedding\",\n",
    "    text_node_property=\"plot\",\n",
    ")\n",
    "\n",
    "plot_retriever = RetrievalQA.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=movie_plot_vector.as_retriever()\n",
    ")\n",
    "\n",
    "response = plot_retriever.invoke(\n",
    "    {\"query\": \"A movie where a mission to the moon goes wrong\"}\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_community.tools import YouTubeSearchTool\n",
    "from langchain_neo4j import Neo4jChatMessageHistory, Neo4jGraph, Neo4jVector\n",
    "from uuid import uuid4\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SESSION_ID = str(uuid4())\n",
    "print(f\"Session ID: {SESSION_ID}\")\n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "embedding_provider = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7476\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"onekenoby\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a movie expert. You find movies from a genre or plot.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_chat = prompt | llm | StrOutputParser()\n",
    "\n",
    "youtube = YouTubeSearchTool()\n",
    "\n",
    "movie_plot_vector = Neo4jVector.from_existing_index(\n",
    "    embedding_provider,\n",
    "    graph=graph,\n",
    "    index_name=\"moviePlots\",\n",
    "    embedding_node_property=\"plotEmbedding\",\n",
    "    text_node_property=\"plot\",\n",
    ")\n",
    "\n",
    "plot_retriever = RetrievalQA.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=movie_plot_vector.as_retriever()\n",
    ")\n",
    "\n",
    "def get_memory(session_id):\n",
    "    return Neo4jChatMessageHistory(session_id=session_id, graph=graph)\n",
    "\n",
    "def call_trailer_search(input):\n",
    "    input = input.replace(\",\", \" \")\n",
    "    return youtube.run(input)\n",
    "\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        name=\"Movie Chat\",\n",
    "        description=\"For when you need to chat about movies. The question will be a string. Return a string.\",\n",
    "        func=movie_chat.invoke,\n",
    "    ),\n",
    "    Tool.from_function(\n",
    "        name=\"Movie Trailer Search\",\n",
    "        description=\"Use when needing to find a movie trailer. The question will include the word trailer. Return a link to a YouTube video.\",\n",
    "        func=call_trailer_search,\n",
    "    ),\n",
    "    Tool.from_function(\n",
    "        name=\"Movie Plot Search\",\n",
    "        description=\"For when you need to compare a plot to a movie. The question will be a string. Return a string.\",\n",
    "        func=plot_retriever.invoke,\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent_prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "agent = create_react_agent(llm, tools, agent_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "chat_agent = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    get_memory,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "while True:\n",
    "    q = input(\"> \")\n",
    "\n",
    "    response = chat_agent.invoke(\n",
    "        {\n",
    "            \"input\": q\n",
    "        },\n",
    "        {\"configurable\": {\"session_id\": SESSION_ID}},\n",
    "    )\n",
    "    \n",
    "    print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (m:Movie {title: \"The Matrix\"})\n",
      "RETURN m.tagline\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'m.tagline': 'Welcome to the Real World'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the plot of the movie The Matrix?',\n",
       " 'result': \"I don't know the answer.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_neo4j import GraphCypherQAChain, Neo4jGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"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\"\n",
    ")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7476\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"onekenoby\",\n",
    ")\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"\n",
    "You are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\n",
    "Convert the user's question based on the schema.\n",
    "\n",
    "Schema: {schema}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "cypher_generation_prompt = PromptTemplate(\n",
    "    template=CYPHER_GENERATION_TEMPLATE,\n",
    "    input_variables=[\"schema\", \"question\"],\n",
    ")\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    llm,\n",
    "    graph=graph,\n",
    "    cypher_prompt=cypher_generation_prompt,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "cypher_chain.invoke({\"query\": \"What is the plot of the movie The Matrix?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[r:ACTED_IN]->(m:Movie)\n",
      "WHERE m.title = \"The Matrix\"\n",
      "RETURN p.name, r.roles\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'p.name': 'Keanu Reeves', 'r.roles': ['Neo']}, {'p.name': 'Carrie-Anne Moss', 'r.roles': ['Trinity']}, {'p.name': 'Laurence Fishburne', 'r.roles': ['Morpheus']}, {'p.name': 'Hugo Weaving', 'r.roles': ['Agent Smith']}, {'p.name': 'Emil Eifrem', 'r.roles': ['Emil']}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Who acted in The Matrix and what roles did they play?',\n",
       " 'result': 'Keanu Reeves acted as Neo, Carrie-Anne Moss acted as Trinity, Laurence Fishburne acted as Morpheus, Hugo Weaving acted as Agent Smith, and Emil Eifrem acted in The Matrix.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_neo4j import GraphCypherQAChain, Neo4jGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"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\"\n",
    ")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7476\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"onekenoby\",\n",
    ")\n",
    "\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"\n",
    "You are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\n",
    "Convert the user's question based on the schema.\n",
    "\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "\n",
    "Schema: {schema}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cypher_generation_prompt = PromptTemplate(\n",
    "    template=CYPHER_GENERATION_TEMPLATE,\n",
    "    input_variables=[\"schema\", \"question\"],\n",
    ")\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    llm,\n",
    "    graph=graph,\n",
    "    cypher_prompt=cypher_generation_prompt,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "cypher_chain.invoke({\"query\": \"Who acted in The Matrix and what roles did they play?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[r:ACTED_IN]->(m:Movie)\n",
      "WHERE m.title = \"Matrix, The\"\n",
      "RETURN p.name, r.roles\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Who acted in The Matrix and what roles did they play?',\n",
       " 'result': \"I don't know the answer.\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_neo4j import GraphCypherQAChain, Neo4jGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"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\"\n",
    ")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7476\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"onekenoby\",\n",
    ")\n",
    "\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"\n",
    "You are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\n",
    "Convert the user's question based on the schema.\n",
    "\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "For movie titles that begin with \"The\", move \"the\" to the end, For example \"The 39 Steps\" becomes \"39 Steps, The\" or \"The Matrix\" becomes \"Matrix, The\".\n",
    "\n",
    "Schema: {schema}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cypher_generation_prompt = PromptTemplate(\n",
    "    template=CYPHER_GENERATION_TEMPLATE,\n",
    "    input_variables=[\"schema\", \"question\"],\n",
    ")\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    llm,\n",
    "    graph=graph,\n",
    "    cypher_prompt=cypher_generation_prompt,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "cypher_chain.invoke({\"query\": \"Who acted in The Matrix and what roles did they play?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[a:ACTED_IN]->(m:Movie)\n",
      "WHERE m.title = \"Matrix, The\"\n",
      "RETURN p.name, a.roles\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Who acted in The Matrix and what roles did they play?',\n",
       " 'result': \"I don't know the answer.\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_neo4j import GraphCypherQAChain, Neo4jGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"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\"\n",
    ")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7476\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"onekenoby\",\n",
    ")\n",
    "\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"\n",
    "You are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\n",
    "Convert the user's question based on the schema.\n",
    "\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "For movie titles that begin with \"The\", move \"the\" to the end, For example \"The 39 Steps\" becomes \"39 Steps, The\" or \"The Matrix\" becomes \"Matrix, The\".\n",
    "\n",
    "If no data is returned, do not attempt to answer the question.\n",
    "Only respond to questions that require you to construct a Cypher statement.\n",
    "Do not include any explanations or apologies in your responses.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Find movies and genres:\n",
    "MATCH (m:Movie)-[:IN_GENRE]->(g)\n",
    "RETURN m.title, g.name\n",
    "\n",
    "Schema: {schema}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cypher_generation_prompt = PromptTemplate(\n",
    "    template=CYPHER_GENERATION_TEMPLATE,\n",
    "    input_variables=[\"schema\", \"question\"],\n",
    ")\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    llm,\n",
    "    graph=graph,\n",
    "    cypher_prompt=cypher_generation_prompt,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "cypher_chain.invoke({\"query\": \"Who acted in The Matrix and what roles did they play?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tx.run(\"\"\"\n",
    "MATCH path = (person:Person)-[actedIn:ACTED_IN]->(movie:Movie {title: $title})\n",
    "RETURN path, person, actedIn, movie\n",
    "\"\"\", title=movie)\n",
    "\n",
    "\n",
    "for record in result:\n",
    "    node = record[\"movie\"]\n",
    "    \n",
    "print(node.id)              # (1)\n",
    "print(node.labels)          # (2)\n",
    "print(node.items())         # (3)\n",
    "\n",
    "# (4)\n",
    "print(node[\"name\"])\n",
    "print(node.get(\"name\", \"N/A\"))\n",
    "\n",
    "acted_in = record[\"actedIn\"]\n",
    "\n",
    "print(acted_in.id)         # (1)\n",
    "print(acted_in.type)       # (2)\n",
    "print(acted_in.items())    # (3)\n",
    "\n",
    "# 4\n",
    "print(acted_in[\"roles\"])\n",
    "print(acted_in.get(\"roles\", \"(Unknown)\"))\n",
    "\n",
    "print(acted_in.start_node) # (5)\n",
    "print(acted_in.end_node)   # (6)\n",
    "\n",
    "path = record[\"path\"]\n",
    "\n",
    "print(path.start_node)  # (1)\n",
    "print(path.end_node)    # (2)\n",
    "print(len(path))  # (1)\n",
    "print(path.relationships)  # (1)\n",
    "\n",
    "for rel in iter(path):\n",
    "    print(rel.type)\n",
    "    print(rel.start_node)\n",
    "    print(rel.end_node)\n",
    "\n",
    "\n",
    "# Create a DateTime instance using individual values\n",
    "datetime = neo4j.time.DateTime(year, month, day, hour, minute, second, nanosecond)\n",
    "\n",
    "#  Create a DateTime  a time stamp (seconds since unix epoch).\n",
    "from_timestamp = neo4j.time.DateTime(1609459200000) # 2021-01-01\n",
    "\n",
    "# Get the current date and time.\n",
    "now = neo4j.time.DateTime.now()\n",
    "\n",
    "print(now.year) # 2022\n",
    "\n",
    "# Using X and Y values\n",
    "twoD=CartesianPoint((1.23, 4.56))\n",
    "print(twoD.x, twoD.y)\n",
    "\n",
    "# Using X, Y and Z\n",
    "threeD=CartesianPoint((1.23, 4.56, 7.89))\n",
    "print(threeD.x, threeD.y, threeD.z)\n",
    "\n",
    "london=WGS84Point((-0.118092, 51.509865))\n",
    "print(london.longitude, london.latitude)\n",
    "\n",
    "the_shard=WGS84Point((-0.086500, 51.504501, 310))\n",
    "print(the_shard.longitude, the_shard.latitude, the_shard.height)\n",
    "\n",
    "WITH point({x: 1, y:1}) AS one,\n",
    "     point({x: 10, y: 10}) AS two\n",
    "\n",
    "RETURN point.distance(one, two) // 12.727922061357855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@movie_routes.get('/')\n",
    "@jwt_required(optional=True)\n",
    "def get_movies():\n",
    "    # Extract pagination values from the request\n",
    "    sort = request.args.get(\"sort\", \"title\")\n",
    "    order = request.args.get(\"order\", \"ASC\")\n",
    "    limit = request.args.get(\"limit\", 6, type=int)\n",
    "    skip = request.args.get(\"skip\", 0, type=int)\n",
    "\n",
    "    # Get User ID from JWT Auth\n",
    "    user_id = current_user[\"sub\"] if current_user != None else None\n",
    "\n",
    "    # Create a new MovieDAO Instance\n",
    "    dao = MovieDAO(current_app.driver)\n",
    "\n",
    "    # Retrieve a paginated list of movies\n",
    "    output = dao.all(sort, order, limit=limit, skip=skip, user_id=user_id)\n",
    "\n",
    "    # Return as JSON\n",
    "    return jsonify(output)\n",
    "\n",
    "\n",
    "def all(self, sort, order, limit=6, skip=0, user_id=None):\n",
    "    # TODO: Get list from movies from Neo4j\n",
    "    return popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGE (:Wallet {address: \"0x111\"});\n",
    "MERGE (:Wallet {address: \"0x222\"});\n",
    "MERGE (:Wallet {address: \"0x333\"});\n",
    "\n",
    "CREATE (:Transaction {hash: \"0xtx1\", value: 2.5});\n",
    "CREATE (:Transaction {hash: \"0xtx2\", value: 1.0});\n",
    "CREATE (:Transaction {hash: \"0xtx3\", value: 0});\n",
    "\n",
    "MERGE (:Block {hash: \"0xabc123\", number: 123456});\n",
    "CREATE (:SmartContract {address: \"0xcontract1\", type: \"ERC20\"});\n",
    "CREATE (:Token {symbol: \"USDC\"});\n",
    "\n",
    "// === Relationships ===\n",
    "// Transaction 0xtx1: 0x111 → 0x222\n",
    "MATCH (from:Wallet {address: \"0x111\"}), (to:Wallet {address: \"0x222\"}), \n",
    "      (tx:Transaction {hash: \"0xtx1\"}), (block:Block {hash: \"0xabc123\"})\n",
    "MERGE (from)-[:SENT]->(tx)\n",
    "MERGE (tx)-[:TO]->(to)\n",
    "MERGE (tx)-[:IN_BLOCK]->(block);\n",
    "\n",
    "// Transaction 0xtx2: 0x222 → 0x333\n",
    "MATCH (from:Wallet {address: \"0x222\"}), (to:Wallet {address: \"0x333\"}), \n",
    "      (tx:Transaction {hash: \"0xtx2\"}), (block:Block {hash: \"0xabc123\"})\n",
    "MERGE (from)-[:SENT]->(tx)\n",
    "MERGE (tx)-[:TO]->(to)\n",
    "MERGE (tx)-[:IN_BLOCK]->(block);\n",
    "\n",
    "// Transaction 0xtx3: 0x222 → SmartContract\n",
    "MATCH (from:Wallet {address: \"0x222\"}), (sc:SmartContract {address: \"0xcontract1\"}), \n",
    "      (tx:Transaction {hash: \"0xtx3\"}), (block:Block {hash: \"0xabc123\"})\n",
    "MERGE (from)-[:SENT]->(tx)\n",
    "MERGE (tx)-[:CALLED]->(sc)\n",
    "MERGE (tx)-[:IN_BLOCK]->(block);\n",
    "\n",
    "// Smart contract deployment\n",
    "MATCH (w:Wallet {address: \"0x333\"}), (sc:SmartContract {address: \"0xcontract1\"})\n",
    "MERGE (w)-[:DEPLOYED]->(sc);\n",
    "\n",
    "// Token holding\n",
    "MATCH (w:Wallet {address: \"0x111\"}), (t:Token {symbol: \"USDC\"})\n",
    "MERGE (w)-[h:HOLDS]->(t)\n",
    "SET h.amount = 150;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyimage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
